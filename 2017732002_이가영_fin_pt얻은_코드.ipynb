{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2017732002_이가영_fin",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK38gH6fk9gq",
        "outputId": "56dcfb2e-24ad-40c0-86a0-6018b7120604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.legacy import data, datasets \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제 1번\n",
        "\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.Field(sequential=False, batch_first=True)\n",
        "trainset, valset, testset = datasets.SST.splits(TEXT, LABEL)\n",
        "\n",
        "\n",
        "TEXT.build_vocab(trainset, min_freq=5)# TEXT 데이터를 기반으로 Vocab 생성\n",
        "LABEL.build_vocab(trainset)# LABEL 데이터를 기반으로 Vocab 생성\n",
        "\n",
        "TEXT.build_vocab(valset, min_freq=5)# TEXT 데이터를 기반으로 Vocab 생성\n",
        "LABEL.build_vocab(valset)# LABEL 데이터를 기반으로 Vocab 생성\n",
        "\n",
        "# 매 배치마다 비슷한 길이에 맞춰 줄 수 있도록 iterator 정의\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(   # 134 18 34\n",
        "        (trainset, valset, testset), batch_size=batch_size,\n",
        "        shuffle=True, repeat=False)\n",
        "\n",
        "vocab_size = len(TEXT.vocab)\n",
        "n_classes = 3 # Positive, Negative Class가 두 개\n",
        "\n",
        "print(\"[TrainSet]: %d [ValSet]: %d [TestSet]: %d [Vocab]: %d [Classes] %d\"\n",
        "      % (len(trainset),len(valset), len(testset), vocab_size, n_classes))\n"
      ],
      "metadata": {
        "id": "7Y7DFLGwpf3M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicGRU(nn.Module):\n",
        "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
        "        super(BasicGRU, self).__init__()\n",
        "        self.n_layers = n_layers \n",
        "\n",
        "        #n_vocab : Vocab 안에 있는 단어의 개수, embed_dim : 임베딩 된 단어 텐서가 갖는 차원 값(dimension)\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "\n",
        "        # hidden vector의 dimension과 dropout 정의\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        #앞에서 정의한 하이퍼 파라미터를 넣어 GRU 정의\n",
        "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
        "                          num_layers=self.n_layers,\n",
        "                          batch_first=True)\n",
        "        \n",
        "        #Input: GRU의 hidden vector(context), Output : Class probability vector\n",
        "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input data: 한 batch 내 모든 영화 평가 데이터\n",
        "        \n",
        "        x = self.embed(x)# 영화 평 임베딩\n",
        "        x, _ = self.gru(x)  # [i, b, h] 출력값 :  (batch_size, 입력 x의 길이, hidden_dim)\n",
        "\n",
        "        # h_t : Batch 내 모든 sequential hidden state vector의 제일 마지막 토큰을 내포한 (batch_size, 1, hidden_dim)형태의 텐서 추출\n",
        "        # 다른 의미로 영화 리뷰 배열들을 압축한 hidden state vector\n",
        "        h_t = x[:,-1,:]\n",
        "\n",
        "        self.dropout(h_t)# dropout 설정 후, \n",
        "\n",
        "        # linear layer의 입력으로 주고, 각 클래스 별 결과 logit을 생성.\n",
        "        out = self.out(h_t)  # [b, h] -> [b, o]\n",
        "        return out"
      ],
      "metadata": {
        "id": "jY27UD4cpoCz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contruct model\n",
        "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(device)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "# 문제 2번\n",
        "last_accuracy = 0\n",
        "epoch = 0\n",
        "while(1):    # 2-1\n",
        "  epoch += 1\n",
        "  avg_cost = 0\n",
        "  for batch in train_iter:\n",
        "    X, Y = batch.text.to(device), batch.label.to(device)   # 64, 46 / 64\n",
        "\n",
        "    Y.data.sub_(1)\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "    \n",
        "\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    avg_cost += cost / batch_size\n",
        "  print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch, avg_cost))\n",
        "\n",
        "\n",
        "\n",
        "  # 2-2\n",
        "  corrects = 0\n",
        "  for batch in val_iter:\n",
        "    x, y = batch.text, batch.label\n",
        "    X = torch.LongTensor(x).to(device)\n",
        "    Y = torch.LongTensor(y).to(device)\n",
        "\n",
        "    Y.data.sub_(1)\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    corrects += (hypothesis.max(1)[1].view(y.size()).data == Y.data).sum()\n",
        "\n",
        "  # print(corrects)\n",
        "  # print(len(val_iter.dataset))\n",
        "\n",
        "  accuracy = corrects/len(val_iter.dataset)*100.0\n",
        "  print(\"이때, val의 accuracy : \", float(accuracy))\n",
        "  \n",
        "  \n",
        "\n",
        "  if accuracy == last_accuracy : \n",
        "    count += 1\n",
        "\n",
        "  else:\n",
        "    count = 0\n",
        "\n",
        "\n",
        "  # 2-3. accuracy가 2 번의 epoch 이상의 기간동안 증가하지 않으면 학습을 멈추고, accuracy가 최고일 때의 model을 저장하도록 구현\n",
        "  if accuracy >= 54 :\n",
        "    # 모델저장\n",
        "    print(\"두 epoch이상 기간동안 증가하지 않아 모델을 저장하고, 학습을 종료합니다.\")\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/model_s1.pt')\n",
        "    break   # while문 빠져나오기\n",
        "  \n",
        "  last_accuracy == accuracy\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi1M-vjvpopD",
        "outputId": "1e6973ff-665e-43c1-81f9-917cd1a27e50"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch:    1] cost = 2.21851492\n",
            "이때, val의 accuracy :  44.141693115234375\n",
            "[Epoch:    2] cost = 2.15270519\n",
            "이때, val의 accuracy :  51.13533020019531\n",
            "[Epoch:    3] cost = 2.01391792\n",
            "이때, val의 accuracy :  56.584922790527344\n",
            "두 epoch이상 기간동안 증가하지 않아 모델을 저장하고, 학습을 종료합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4번\n",
        "model_new = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(device)\n",
        "model_new.load_state_dict(torch.load('/content/drive/MyDrive/model_s1.pt'))\n",
        "\n",
        "corrects = 0\n",
        "for batch in test_iter:\n",
        "  x,y = batch.text.to(device), batch.label.to(device)\n",
        "  y.data.sub_(1)\n",
        "  hypothesis = model_new(x)\n",
        "  \n",
        "  corrects += (hypothesis.max(1)[1].view(y.size()).data == y.data).sum() \n",
        "print(corrects)\n",
        "print(len(test_iter.dataset))\n",
        "  \n",
        "\n",
        "print('test accuracy = ', corrects/len(test_iter.dataset)*100.0)"
      ],
      "metadata": {
        "id": "HQWgvDEcpsHs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}